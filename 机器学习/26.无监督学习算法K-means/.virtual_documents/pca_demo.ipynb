# 第一步: 获取数据
# 第二步: 合并表
# 第三步: 找到user_id和aisle之间的关系
# 第四步: PCA降维


import pandas as pd


# 获取数据
order_products = pd.read_csv("./instacart/order_products__prior.csv")
products = pd.read_csv("./instacart/products.csv")
orders = pd.read_csv("./instacart/orders.csv")
aisles = pd.read_csv("./instacart/aisles.csv")


# 进行合并表
tab1 = pd.merge(aisles,products,on=["aisle_id","aisle_id"]) #默认就是内连接


tab2 = pd.merge(tab1,order_products,on=["product_id","product_id"])


tab3 = pd.merge(tab2,orders,on=["order_id","order_id"])


tab3.head()


# 使用交叉表找到user_id和aisle之间的关系
table = pd.crosstab(tab3["user_id"],tab3["aisle"]) # 新的表会列出同一个用户不同购物通道的频次


data = table[0:10000] # 数据量太大,现在只取前10000条数据


# 通过PCA降维减少数据的冗余 简化数据\减少噪声\提高效率
from sklearn.decomposition import PCA


transfer = PCA(n_components=0.95)  # 比较常用小数
data_new = transfer.fit_transform(data)


data_new.shape  # 发现降维很多 但是信息损失很少


# 没有目标值 所以直接进入预估器流程
from sklearn.cluster import KMeans


estimator = KMeans(n_clusters=3) # 指定分为三簇


estimator.fit(data_new) # 训练模型


y_predict = estimator.predict(data_new)


y_predict[0:300]


# 模型评估 - 轮廓系数
from sklearn.metrics import silhouette_score


silhouette_score(data_new,y_predict) # 0.5396819903993837 还算可以了 范围是-1到1



