### 中文文本特征抽取
参数中有一个stop_words叫做停用词，我们可以把一些不要的词放进去  
很多做自然语言统计的机构就会研究停用词表  
把什么词认为没什么用的 就给加进去  
```python
def count_demo():
    #文本特征抽取
    data = ["I am a cat,i like python!","You are a cat,you do not like python!"]
    #实例化转换器，调用fit_transform
    transfer = CountVectorizer(stop_words=["cat"])
    data_new = transfer.fit_transform(data)
    print(data_new.toarray())
    print("特征名：", transfer.get_feature_names_out())
    return None
```
手动拆开不实用，我们可以用其他方法  
导入jieba  
```python
out = " ".join(list(jieba.cut(text)))
```
```python
def cut_word(text):
    out = " ".join(list(jieba.cut(text)))
    return out

def count_chinese_demo():
    #中文文本特征抽取 自动分词
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    data_new = []
    for sent in data:
        data_new.append(cut_word(sent))
    transfer = CountVectorizer(stop_words=["一种","所以"])
    data_final = transfer.fit_transform(data_new)
    print(data_final.toarray())
    print("特征名：", transfer.get_feature_names_out())
    return None
```

```
C:\Users\shun_\AppData\Local\Programs\Python\Python310\python.exe C:/Users/shun_/Desktop/Project/Public/MachineLearning/机器学习/代码/demo01/day01.py
Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\shun_\AppData\Local\Temp\jieba.cache
Loading model cost 0.549 seconds.
[[0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0]
 [0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1]
Prefix dict has been built successfully.
 [1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]]
特征名： ['不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'
 '大部分' '如何' '如果' '宇宙' '我们' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个' '看到'
 '真正' '秘密' '绝对' '美好' '联系' '过去' '还是' '这样']

进程已结束,退出代码0

```
